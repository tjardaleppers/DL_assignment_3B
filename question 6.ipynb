{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget, os, gzip, pickle, random, re, sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "\n",
    "IMDB_URL = 'http://dlvu.github.io/data/imdb.{}.pkl.gz'\n",
    "IMDB_FILE = 'imdb.{}.pkl.gz'\n",
    "\n",
    "PAD, START, END, UNK = '.pad', '.start', '.end', '.unk'\n",
    "\n",
    "def load_imdb(final=False, val=5000, seed=0, voc=None, char=False):\n",
    "\n",
    "    cst = 'char' if char else 'word'\n",
    "\n",
    "    imdb_url = IMDB_URL.format(cst)\n",
    "    imdb_file = IMDB_FILE.format(cst)\n",
    "\n",
    "    if not os.path.exists(imdb_file):\n",
    "        wget.download(imdb_url)\n",
    "\n",
    "    with gzip.open(imdb_file) as file:\n",
    "        sequences, labels, i2w, w2i = pickle.load(file)\n",
    "\n",
    "    if voc is not None and voc < len(i2w):\n",
    "        nw_sequences = {}\n",
    "\n",
    "        i2w = i2w[:voc]\n",
    "        w2i = {w: i for i, w in enumerate(i2w)}\n",
    "\n",
    "        mx, unk = voc, w2i['.unk']\n",
    "        for key, seqs in sequences.items():\n",
    "            nw_sequences[key] = []\n",
    "            for seq in seqs:\n",
    "                seq = [s if s < mx else unk for s in seq]\n",
    "                nw_sequences[key].append(seq)\n",
    "\n",
    "        sequences = nw_sequences\n",
    "\n",
    "    if final:\n",
    "        return (sequences['train'], labels['train']), (sequences['test'], labels['test']), (i2w, w2i), 2\n",
    "\n",
    "    # Make a validation split\n",
    "    random.seed(seed)\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    x_val, y_val = [], []\n",
    "\n",
    "    val_ind = set( random.sample(range(len(sequences['train'])), k=val) )\n",
    "    for i, (s, l) in enumerate(zip(sequences['train'], labels['train'])):\n",
    "        if i in val_ind:\n",
    "            x_val.append(s)\n",
    "            y_val.append(l)\n",
    "        else:\n",
    "            x_train.append(s)\n",
    "            y_train.append(l)\n",
    "\n",
    "    return (x_train, y_train), \\\n",
    "           (x_val, y_val), \\\n",
    "           (i2w, w2i), 2\n",
    "\n",
    "\n",
    "def gen_sentence(sent, g):\n",
    "\n",
    "    symb = '_[a-z]*'\n",
    "\n",
    "    while True:\n",
    "\n",
    "        match = re.search(symb, sent)\n",
    "        if match is None:\n",
    "            return sent\n",
    "\n",
    "        s = match.span()\n",
    "        sent = sent[:s[0]] + random.choice(g[sent[s[0]:s[1]]]) + sent[s[1]:]\n",
    "\n",
    "def gen_dyck(p):\n",
    "    open = 1\n",
    "    sent = '('\n",
    "    while open > 0:\n",
    "        if random.random() < p:\n",
    "            sent += '('\n",
    "            open += 1\n",
    "        else:\n",
    "            sent += ')'\n",
    "            open -= 1\n",
    "\n",
    "    return sent\n",
    "\n",
    "def gen_ndfa(p):\n",
    "\n",
    "    word = random.choice(['abc!', 'uvw!', 'klm!'])\n",
    "\n",
    "    s = ''\n",
    "    while True:\n",
    "        if random.random() < p:\n",
    "            return 's' + s + 's'\n",
    "        else:\n",
    "            s+= word\n",
    "\n",
    "def load_brackets(n=50_000, seed=0):\n",
    "    return load_toy(n, char=True, seed=seed, name='dyck')\n",
    "\n",
    "def load_ndfa(n=50_000, seed=0):\n",
    "    return load_toy(n, char=True, seed=seed, name='ndfa')\n",
    "\n",
    "def load_toy(n=50_000, char=True, seed=0, name='lang'):\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    if name == 'lang':\n",
    "        sent = '_s'\n",
    "\n",
    "        toy = {\n",
    "            '_s': ['_s _adv', '_np _vp', '_np _vp _prep _np', '_np _vp ( _prep _np )', '_np _vp _con _s' , '_np _vp ( _con _s )'],\n",
    "            '_adv': ['briefly', 'quickly', 'impatiently'],\n",
    "            '_np': ['a _noun', 'the _noun', 'a _adj _noun', 'the _adj _noun'],\n",
    "            '_prep': ['on', 'with', 'to'],\n",
    "            '_con' : ['while', 'but'],\n",
    "            '_noun': ['mouse', 'bunny', 'cat', 'dog', 'man', 'woman', 'person'],\n",
    "            '_vp': ['walked', 'walks', 'ran', 'runs', 'goes', 'went'],\n",
    "            '_adj': ['short', 'quick', 'busy', 'nice', 'gorgeous']\n",
    "        }\n",
    "\n",
    "        sentences = [ gen_sentence(sent, toy) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s : len(s))\n",
    "\n",
    "    elif name == 'dyck':\n",
    "\n",
    "        sentences = [gen_dyck(7./16.) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s: len(s))\n",
    "\n",
    "    elif name == 'ndfa':\n",
    "\n",
    "        sentences = [gen_ndfa(1./4.) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s: len(s))\n",
    "\n",
    "    else:\n",
    "        raise Exception(name)\n",
    "\n",
    "    tokens = set()\n",
    "    for s in sentences:\n",
    "\n",
    "        if char:\n",
    "            for c in s:\n",
    "                tokens.add(c)\n",
    "        else:\n",
    "            for w in s.split():\n",
    "                tokens.add(w)\n",
    "\n",
    "    i2t = [PAD, START, END, UNK] + list(tokens)\n",
    "    t2i = {t:i for i, t in enumerate(i2t)}\n",
    "\n",
    "    sequences = []\n",
    "    for s in sentences:\n",
    "        if char:\n",
    "            tok = list(s)\n",
    "        else:\n",
    "            tok = s.split()\n",
    "        sequences.append([t2i[t] for t in tok])\n",
    "\n",
    "    return sequences, (i2t, t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#x_train, (i2w, w2i) = load_ndfa(n=150_000)\n",
    "\n",
    "x_train, (i2w, w2i) = load_brackets(n=150_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.pad': 0, '.start': 1, '.end': 2, '.unk': 3, ')': 4, '(': 5}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-75907c95c6fa>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_batches = [torch.tensor(batch, dtype=torch.long) for batch in padded_batches]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[1, 5, 4, 2],\n",
       "         [1, 5, 4, 2],\n",
       "         [1, 5, 4, 2],\n",
       "         ...,\n",
       "         [1, 5, 4, 2],\n",
       "         [1, 5, 4, 2],\n",
       "         [1, 5, 4, 2]]),\n",
       " tensor([[1, 5, 4, 2],\n",
       "         [1, 5, 4, 2],\n",
       "         [1, 5, 4, 2],\n",
       "         ...,\n",
       "         [1, 5, 4, 2],\n",
       "         [1, 5, 4, 2],\n",
       "         [1, 5, 4, 2]]),\n",
       " tensor([[1, 5, 4, 2],\n",
       "         [1, 5, 4, 2],\n",
       "         [1, 5, 4, 2],\n",
       "         ...,\n",
       "         [1, 5, 4, 2],\n",
       "         [1, 5, 4, 2],\n",
       "         [1, 5, 4, 2]]),\n",
       " tensor([[1, 5, 4, 2],\n",
       "         [1, 5, 4, 2],\n",
       "         [1, 5, 4, 2],\n",
       "         ...,\n",
       "         [1, 5, 4, 2],\n",
       "         [1, 5, 4, 2],\n",
       "         [1, 5, 4, 2]]),\n",
       " tensor([[1, 5, 4, 2],\n",
       "         [1, 5, 4, 2],\n",
       "         [1, 5, 4, 2],\n",
       "         ...,\n",
       "         [1, 5, 4, 2],\n",
       "         [1, 5, 4, 2],\n",
       "         [1, 5, 4, 2]]),\n",
       " tensor([[1, 5, 4, 2],\n",
       "         [1, 5, 4, 2],\n",
       "         [1, 5, 4, 2],\n",
       "         ...,\n",
       "         [1, 5, 4, 2],\n",
       "         [1, 5, 4, 2],\n",
       "         [1, 5, 4, 2]]),\n",
       " tensor([[1, 5, 4, 2, 0, 0],\n",
       "         [1, 5, 4, 2, 0, 0],\n",
       "         [1, 5, 4, 2, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5, 4, 4, 2],\n",
       "         [1, 5, 5, 4, 4, 2],\n",
       "         [1, 5, 5, 4, 4, 2]]),\n",
       " tensor([[1, 5, 5, 4, 4, 2],\n",
       "         [1, 5, 5, 4, 4, 2],\n",
       "         [1, 5, 5, 4, 4, 2],\n",
       "         ...,\n",
       "         [1, 5, 5, 4, 4, 2],\n",
       "         [1, 5, 5, 4, 4, 2],\n",
       "         [1, 5, 5, 4, 4, 2]]),\n",
       " tensor([[1, 5, 5, 4, 4, 2],\n",
       "         [1, 5, 5, 4, 4, 2],\n",
       "         [1, 5, 5, 4, 4, 2],\n",
       "         ...,\n",
       "         [1, 5, 5, 4, 4, 2],\n",
       "         [1, 5, 5, 4, 4, 2],\n",
       "         [1, 5, 5, 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 2, 0, 0],\n",
       "         [1, 5, 5,  ..., 2, 0, 0],\n",
       "         [1, 5, 5,  ..., 2, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 2, 0, 0],\n",
       "         [1, 5, 5,  ..., 2, 0, 0],\n",
       "         [1, 5, 5,  ..., 2, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 2, 0, 0],\n",
       "         [1, 5, 5,  ..., 2, 0, 0],\n",
       "         [1, 5, 5,  ..., 2, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 2, 0, 0],\n",
       "         [1, 5, 5,  ..., 2, 0, 0],\n",
       "         [1, 5, 5,  ..., 2, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 2, 0, 0],\n",
       "         [1, 5, 5,  ..., 2, 0, 0],\n",
       "         [1, 5, 5,  ..., 2, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]]),\n",
       " tensor([[1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 0, 0, 0],\n",
       "         [1, 5, 5,  ..., 4, 4, 2]])]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Add start and end tokens to each sequence\n",
    "x_train_preprocessed = [[w2i['.start']] + seq + [w2i['.end']] for seq in x_train]\n",
    "\n",
    "\n",
    "# Set a maximum number of tokens per batch\n",
    "max_tokens_per_batch = 50000  # Adjust this value as needed\n",
    "\n",
    "# Split data into batches\n",
    "batches = []\n",
    "current_batch = []\n",
    "current_batch_tokens = 0\n",
    "\n",
    "for seq in x_train_preprocessed:\n",
    "    # Check if adding the sequence exceeds the maximum tokens per batch\n",
    "    if current_batch_tokens + len(seq) <= max_tokens_per_batch:\n",
    "        current_batch.append(seq)\n",
    "        current_batch_tokens += len(seq)\n",
    "    else:\n",
    "        batches.append(current_batch)\n",
    "        current_batch = [seq]\n",
    "        current_batch_tokens = len(seq)\n",
    "\n",
    "# Add the last batch\n",
    "if current_batch:\n",
    "    batches.append(current_batch)\n",
    "\n",
    "# Now, 'batches' contains lists of sequences with start and end tokens, respecting the maximum tokens per batch\n",
    "\n",
    "# Pad sequences within each batch to the same length\n",
    "padded_batches = [pad_sequence([torch.tensor(s) for s in batch], batch_first=True, padding_value=w2i['.pad']) for batch in batches]\n",
    "\n",
    "# Convert lists of batches to PyTorch tensors\n",
    "tensor_batches = [torch.tensor(batch, dtype=torch.long) for batch in padded_batches]\n",
    "\n",
    "# Now, 'tensor_batches' contains tensors of sequences with start and end tokens, all with the same length\n",
    "\n",
    "# Example: Print the first batch\n",
    "tensor_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 4, 2, 0],\n",
      "        [5, 4, 2, 0],\n",
      "        [5, 4, 2, 0],\n",
      "        ...,\n",
      "        [5, 4, 2, 0],\n",
      "        [5, 4, 2, 0],\n",
      "        [5, 4, 2, 0]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove the first column (start token) from each sequence in tensor_batches to get target_batches\n",
    "target_batches = [batch[:, 1:] for batch in tensor_batches]\n",
    "\n",
    "# Append a column of zeros to each sequence in target_batches\n",
    "target_batches = [torch.cat((batch, torch.zeros(batch.size(0), 1, dtype=torch.long)), dim=1) for batch in target_batches]\n",
    "\n",
    "# Now, target_batches contains tensors of sequences with the same length as input sequences, but shifted one token to the left\n",
    "\n",
    "# Example: Print the first target batch\n",
    "print(target_batches[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.3987\n",
      "Epoch [2/10], Loss: 1.2094\n",
      "Epoch [3/10], Loss: 1.0598\n",
      "Epoch [4/10], Loss: 0.9559\n",
      "Epoch [5/10], Loss: 0.8993\n",
      "Epoch [6/10], Loss: 0.8691\n",
      "Epoch [7/10], Loss: 0.8503\n",
      "Epoch [8/10], Loss: 0.8368\n",
      "Epoch [9/10], Loss: 0.8250\n",
      "Epoch [10/10], Loss: 0.8148\n"
     ]
    }
   ],
   "source": [
    "# Define the LSTM model\n",
    "class NDFA_LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
    "        super(NDFA_LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.lstm = nn.LSTM(input_size=emb_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        output = self.linear(lstm_out)\n",
    "        return output\n",
    "\n",
    "# Initialize the model\n",
    "vocab_size = len(i2w)\n",
    "emb_size = 32\n",
    "hidden_size = 16\n",
    "\n",
    "model = NDFA_LSTM(vocab_size, emb_size, hidden_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Assuming 0 represents the padding index\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "#tensor_batches = [torch.tensor(batch, dtype=torch.long) for batch in padded_batches]\n",
    "#target_batches = [torch.tensor(batch, dtype=torch.long) for batch in target_batches]\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for input_batch, target_batch in zip(tensor_batches, target_batches):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(input_batch)\n",
    "\n",
    "        # Reshape output and target to (batch_size * sequence_length, vocab_size)\n",
    "        output = output.view(-1, vocab_size)\n",
    "        target = target_batch.view(-1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'> torch.Size([6, 32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64, 32])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64, 16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([6, 16])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([6])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fec8293d88d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Assuming tensor_batches and target_batches for training and validation are already prepared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpackaging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m if not hasattr(tensorboard, \"__version__\") or Version(\n\u001b[1;32m      5\u001b[0m     \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Assuming tensor_batches and target_batches for training and validation are already prepared\n",
    "\n",
    "# Convert lists of batches to PyTorch tensors\n",
    "tensor_batches_train = [torch.tensor(batch, dtype=torch.long) for batch in padded_batches]\n",
    "target_batches_train = [torch.tensor(batch, dtype=torch.long) for batch in target_batches]\n",
    "\n",
    "#tensor_batches_val = [torch.tensor(batch, dtype=torch.long) for batch in padded_batches_val]\n",
    "#target_batches_val = [torch.tensor(batch, dtype=torch.long) for batch in target_batches_val]\n",
    "\n",
    "# Define the LSTM model\n",
    "class NDFA_LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, num_layers=1, dropout=0.0):\n",
    "        super(NDFA_LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.lstm = nn.LSTM(input_size=emb_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        output = self.linear(lstm_out)\n",
    "        return output\n",
    "\n",
    "# Initialize the model\n",
    "vocab_size = len(i2w)\n",
    "emb_size = 32\n",
    "hidden_size = 16\n",
    "\n",
    "model = NDFA_LSTM(vocab_size, emb_size, hidden_size)\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')  # Assuming 0 represents the padding index\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Set up TensorBoard for visualization\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50  # Adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    total_norm = 0\n",
    "\n",
    "    for input_batch, target_batch in zip(tensor_batches_train, target_batches_train):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(input_batch)\n",
    "\n",
    "        # Reshape output and target to (batch_size * sequence_length, vocab_size)\n",
    "        output = output.view(-1, vocab_size)\n",
    "        target = target_batch.view(-1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track total tokens and loss\n",
    "        total_tokens += target_batch.numel()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Track gradient norm\n",
    "        total_norm += nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    average_train_loss = total_loss / total_tokens\n",
    "\n",
    "    # Log training loss, gradient norm, and epoch\n",
    "    writer.add_scalar('Train/Loss', average_train_loss, epoch)\n",
    "    writer.add_scalar('Train/Gradient_Norm', total_norm, epoch)\n",
    "\n",
    "    print(f'Training Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_train_loss:.4f}, Average Gradient Norm: {total_norm:.4f}')\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Samples after Epoch 1:\n",
      ".start ( ( ) ) .end\n",
      ".start ( ( ) ( ) ( ( ( ( ) ( ) ( ) .end\n",
      ".start ( ( ) ( ) ( ) ( ( ( ) ( ( ( ) ( ) ( ) ) ) ( ( ( ( ( ( ( ) ( ) ) ( ) ( ) ) ( ( ) ( ) (\n",
      ".start ( ( ) ) ( ) ( ) ( ) .end\n",
      ".start ( ( ) .end\n",
      ".start ( ( ) ( ( ) ( ( ( ( ) ( ) ( ) ( ( ) .end\n",
      ".start ( ( ) ( ) ( ) ( ) ( ( ( ) ( ) .end\n",
      ".start ( ( ) .end\n",
      ".start ( ( ) ) ( ) ( ) ( ) ( ) ( ( ) ( ( ) ( ) ( ) ( ) .end\n",
      ".start ( ( ) ( ) ( ( ) ) .end\n",
      "Generated Samples after Epoch 2:\n",
      ".start ( ( ) .end\n",
      ".start ( ( ) ( ) ( ) .end\n",
      ".start ( ( ) .end\n",
      ".start ( ( ) ) ( ( ( ) .end\n",
      ".start ( ( ) ( ( ) .end\n",
      ".start ( ( ) ( ) ( ( ( ) ( ( ( ) ( ( ( ) .end\n",
      ".start ( ( ) .end\n",
      ".start ( ( ) ( ) ( ( ( ) .end\n",
      ".start ( ( ) ( ) ( ) ( ) .end\n",
      ".start ( ( ) .end\n",
      "Generated Samples after Epoch 3:\n",
      ".start ( ( ) ( ) ( ) ( ( ( ) ( ) ( ( ) ( ) ( ( ( ( ) ( ) ( ) .end\n",
      ".start ( ( ) ( ) ( ( ) ( ( ( ) .end\n",
      ".start ( ( ) ( ( ( ) ( ) .end\n",
      ".start ( ( ) ( ) ( ( ( ( ( ) ( ) ( ) .end\n",
      ".start ( ( ) ( ) ( ( ( ) ( ) ( ( ( ( ) ( ( ) ( ( ) ( ( ) ) .end\n",
      ".start ( ( ) ( ) ( ) ( ) ( ) ( ) ) ( ( ( ( ) ( ) .end\n",
      ".start ( ( ) ( ) ( ( ( ( ( ) .end\n",
      ".start ( ( ) ( ) ) ( ( ) ( ( ) ( ) ( ) ( ) .end\n",
      ".start ( ( ) ( ) .end\n",
      ".start ( ( ) ( ) ) ( ) ( ) ( ( ) ( ) ) ( ) ( ( ) .end\n",
      "Generated Samples after Epoch 4:\n",
      ".start ( ( ) ( ( ) ( ) ( ( ) ( ) ( ) ( ( ( ) ( ) .end\n",
      ".start ( ( ) .end\n",
      ".start ( ( ) ( ( ) ( ( ) ( ) ( ( ) ( ( ( ( ( ) ( ( ) ( ) .end\n",
      ".start ( ( ) ( ( ) ( ( ) ( ( ( ) ( ( ( ) ( ) .end\n",
      ".start ( ( ) ( ) .end\n",
      ".start ( ( ) ( ) ( ) ( ) ) ( ( ) ( ( ( ( ) ( ) ( ) .end\n",
      ".start ( ( ) ( ) ( ) ( ) .end\n",
      ".start ( ( ) ( ) .end\n",
      ".start ( ( ) ( ) ( ) .end\n",
      ".start ( ( ) .end\n",
      "Generated Samples after Epoch 5:\n",
      ".start ( ( ) ( ) ( ) ( ( ( ) ( ( ( ) ( ( ) ( ) ( ( ) ( ( ( ( ) ( ) ( ) ( ) ( ) ( ) ( ( ( ( )\n",
      ".start ( ( ) ( ) ( ( ) ( ) ( ( ) ( ( ) .end\n",
      ".start ( ( ) .end\n",
      ".start ( ( ) ( ( ( ) ( ) .end\n",
      ".start ( ( ) ( ( ) ( ) ) ( ) ( ) ( ) ( ) ( ( ) ( ( ) ( ) ( ) ( ( ( ) ( ( ) ( ) ) .end\n",
      ".start ( ( ) ( ( ( ) .end\n",
      ".start ( ( ) .end\n",
      ".start ( ( ) ( ) .end\n",
      ".start ( ( ) ( ) ( ) ( ) ( ( ( ) ( ( ) ( ) ( ) ( ( ) ( ) ( ) ( ) ( ) .end\n",
      ".start ( ( ) ( ( ) .end\n",
      "Generated Samples after Epoch 6:\n",
      ".start ( ( ) ( ) ( ) ( ) ( ) .end\n",
      ".start ( ( ) ) .end\n",
      ".start ( ( ) .end\n",
      ".start ( ( ) ( ( ) ( ) ) ( ) .end\n",
      ".start ( ( ) ( ) ( ) ( ) .end\n",
      ".start ( ( ) ( ( ) ( ) ( ) ( ) ( ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) .end\n",
      ".start ( ( ) ( ( ) ( ( ) ( ) ( ) ( ) ( ) ( ( ( ) ( ) .end\n",
      ".start ( ( ) ( ) ( ) ( ) ( ) ( ( ) ( ) .end\n",
      ".start ( ( ) ( ) ( ( ( ) ( ) .end\n",
      ".start ( ( ) ( ) ( ) .end\n",
      "Generated Samples after Epoch 7:\n",
      ".start ( ( ) ( ) ( ) ( ) ) .end\n",
      ".start ( ( ) ( ) ( ) ( ) ) ( ) ( ( ( ( ) ( ) ( ) ( ) ) .end\n",
      ".start ( ( ) ( ) .end\n",
      ".start ( ( ) ( ) .end\n",
      ".start ( ( ) ) ( ) ( ) .end\n",
      ".start ( ( ) .end\n",
      ".start ( ( ) ( ) .end\n",
      ".start ( ( ) ( ) ( ) ( ( ) ( ) ( ) ) ( ( ( ) .end\n",
      ".start ( ( ) ( ) ( ) ( ) ( ( ( ( ( ( ) ( ( ( ) ( ( ) ( ) ) ( ) .end\n",
      ".start ( ( ) ( ( ( ) .end\n",
      "Generated Samples after Epoch 8:\n",
      ".start ( ( ) ( ) .end\n",
      ".start ( ( ) ( ) ( ) ( ) .end\n",
      ".start ( ( ) .end\n",
      ".start ( ( ) ( ) ) ( ) ( ( ( ) ( ) .end\n",
      ".start ( ( ) .end\n",
      ".start ( ( ) .end\n",
      ".start ( ( ) ) ( ( ) .end\n",
      ".start ( ( ) ( ) ( ( ) ) ( ) ( ( ) ( ( ( ) ( ( ( ( ) ( ( ( ) ( ) .end\n",
      ".start ( ( ) ( ) ( ( ) .end\n",
      ".start ( ( ) ( ) ( ) ) ( ( ( ) ) ( ( ) ) ( ) .end\n",
      "Generated Samples after Epoch 9:\n",
      ".start ( ( ) .end\n",
      ".start ( ( ) ( ) ( ) ( ) .end\n",
      ".start ( ( ) ( ) ) ( ( ( ) ( ( ) ( ) ( ( ) .end\n",
      ".start ( ( ) ( ) ( ( ) ( ( ) ( ) ( ) ( ( ) ( ( ( ( ( ) ( ) ( ) .end\n",
      ".start ( ( ) ( ) ( ( ) ( ( ( ( ) ( ( ) ( ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ) .end\n",
      ".start ( ( ) ( ) ( ) ( ) ( ) .end\n",
      ".start ( ( ) ( ) ( ( ( ) ( ) ( ( ) ( ( ) ( ) ( ( ( ) ( ) ) .end\n",
      ".start ( ( ) .end\n",
      ".start ( ( ) ( ) .end\n",
      ".start ( ( ) .end\n",
      "Generated Samples after Epoch 10:\n",
      ".start ( ( ) ) ) .end\n",
      ".start ( ( ) ( ) ( ) .end\n",
      ".start ( ( ) .end\n",
      ".start ( ( ) ( ( ) ( ( ) ( ( ) ( ) ( ) ( ) .end\n",
      ".start ( ( ) ( ) ( ( ) ( ) ( ) ( ) .start ( ( ( ( ) ( ( ) ( ) ( ) ( ) ( ) ( ) ( ( ( ( ( ) ( ( ( )\n",
      ".start ( ( ) ) ( ) ) ( ) ( ) ( ) ( ( ) ( ) ) ( ( ) ) ( ( ) ( ) .end\n",
      ".start ( ( ) ( ) .end\n",
      ".start ( ( ) ( ( ) ( ) ( ) ( ( ) ( ) .end\n",
      ".start ( ( ) ( ) ( ( ) ( ) ( ( ) ( ) ( ( ) .end\n",
      ".start ( ( ) ( ) .end\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "\n",
    "def sample(lnprobs, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample an element from a categorical distribution\n",
    "    :param lnprobs: Outcome logits\n",
    "    :param temperature: Sampling temperature. 1.0 follows the given distribution, 0.0 returns the maximum probability element.\n",
    "    :return: The index of the sampled element.\n",
    "    \"\"\"\n",
    "    if temperature == 0.0:\n",
    "        return lnprobs.argmax()\n",
    "    p = F.softmax(lnprobs / temperature, dim=0)\n",
    "    cd = dist.Categorical(p)\n",
    "    return cd.sample()\n",
    "\n",
    "def generate_sequence(model, seed_sequence, max_length=100, temperature=1.0, num_samples=10):\n",
    "    model.eval()\n",
    "\n",
    "    # Convert seed_sequence to tensor and add a singleton batch dimension\n",
    "    seed_sequence_tensor = torch.tensor([seed_sequence], dtype=torch.long)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Generated Samples after Epoch {epoch + 1}:\")\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            generated_sequence = seed_sequence.copy()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for _ in range(max_length):\n",
    "                    # Forward pass\n",
    "                    output = model(seed_sequence_tensor)\n",
    "\n",
    "                    # Get the last predicted logits\n",
    "                    last_logits = output[0, -1, :]\n",
    "\n",
    "                    # Sample the next token using the provided sample function\n",
    "                    next_token = sample(last_logits, temperature).item()\n",
    "\n",
    "                    # Append the sampled token to the generated sequence\n",
    "                    generated_sequence.append(next_token)\n",
    "\n",
    "                    # Check for the end token\n",
    "                    if next_token == w2i['.end']:\n",
    "                        break\n",
    "\n",
    "                    # Prepare the next input for the model\n",
    "                    seed_sequence_tensor = torch.tensor([[next_token]], dtype=torch.long)\n",
    "\n",
    "            # Convert the generated sequence back to tokens\n",
    "            generated_tokens = [i2w[token] for token in generated_sequence]\n",
    "\n",
    "            # Print the generated sequence\n",
    "            print(\" \".join(generated_tokens))\n",
    "\n",
    "# Example usage:\n",
    "seed_sequence = [w2i['.start'], w2i['('], w2i['('], w2i[')']]\n",
    "generate_sequence(model, seed_sequence, max_length=40, temperature=0.15, num_samples=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
